{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80edaf11-5660-469e-abff-f9e3a2543b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, matthews_corrcoef, auc, accuracy_score, recall_score, precision_score, f1_score\n",
    "import torchvision\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torch import optim, cuda\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "from timeit import default_timer as timer\n",
    "from collections import OrderedDict\n",
    "\n",
    "from hachoir.parser import createParser # pip install --user hachoir\n",
    "from hachoir.metadata import extractMetadata\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787a252-b99f-4e80-9d66-051faa2083ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21383d-6fd8-4dca-b220-71d95194acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('image_table.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f2fec-8bce-47d2-8e48-00a6338b0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgtransResize = (320, 320)\n",
    "#TRANSFORM DATA\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transformList = []\n",
    "transformList.append(transforms.RandomHorizontalFlip())\n",
    "transformList.append(transforms.RandomInvert())\n",
    "transformList.append(transforms.RandomRotation(degrees=(-15, 15), expand=False, interpolation=InterpolationMode.BICUBIC))\n",
    "transformList.append(transforms.ColorJitter(brightness=0.5, contrast=0.5)) # try 0.75 next\n",
    "#transformList.append(transforms.RandAugment(num_ops=2, magnitude=9, interpolation=InterpolationMode.BICUBIC))\n",
    "transformList.append(transforms.Resize(imgtransResize,interpolation=InterpolationMode.BICUBIC))\n",
    "transformList.append(transforms.ToTensor())\n",
    "transformList.append(normalize)      \n",
    "transformSequence=transforms.Compose(transformList)\n",
    "\n",
    "\n",
    "val_transformList = []\n",
    "val_transformList.append(transforms.Resize(imgtransResize,interpolation=InterpolationMode.BICUBIC))\n",
    "val_transformList.append(transforms.ToTensor())\n",
    "val_transformList.append(normalize)\n",
    "val_transformSequence=transforms.Compose(val_transformList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e461da7-a8f2-4bc0-b081-f76aed2bd2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class png_loader(Dataset):\n",
    "    def __init__(self, image_list_file, transform=None):\n",
    "\n",
    "        image_names = []\n",
    "        labels = []\n",
    "\n",
    "        for row, line in image_list_file.iterrows():\n",
    "            image_name = line['png_path']\n",
    "            label = line[y_list]\n",
    "            \n",
    "            for i in range(len(label)):\n",
    "                label[i] = line[y_list[i]]\n",
    "            \n",
    "            image_names.append(image_name)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Take the index of item and returns the image and its labels\"\"\"\n",
    "        \n",
    "        filename = self.image_names[index]\n",
    "\n",
    "        image = Image.open(root_dir + str(filename))\n",
    "        \n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.FloatTensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d52b74-c338-4d9f-be56-65f0d24a9dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = ['bilateral']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a236f4-acfa-447e-b735-8666845aec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_BatchSize = 8\n",
    "nnClassCount = len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46309a7d-14ba-4957-acdc-786c19f78964",
   "metadata": {},
   "outputs": [],
   "source": [
    "empi_list = data_df.empi_anon.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6296f-4d3a-41d0-9836-3d28e4bf86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list = shuffle(empi_list, random_state=42)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd05986-fe9f-47b6-8746-2e4c51ceccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data_df[~data_df.empi_anon.isin(val_list)]\n",
    "validate_df = data_df[data_df.empi_anon.isin(val_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc4f0b-aa1b-43d2-a37f-d8c16945e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTrain = png_loader(train_df, transformSequence)\n",
    "datasetValid = png_loader(validate_df, val_transformSequence)\n",
    "\n",
    "dataLoaderTrain = DataLoader(dataset=datasetTrain, batch_size=training_BatchSize, shuffle=True,  num_workers=32, pin_memory=True, drop_last=True)\n",
    "dataLoaderVal = DataLoader(dataset=datasetValid, batch_size=training_BatchSize, shuffle=False, num_workers=32, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7602d-df1c-4c67-ab86-1e3551230f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Trainer():\n",
    "\n",
    "    def train (model, dataLoaderTrain, dataLoaderVal, nnClassCount, trMaxEpoch, launchTimestamp, checkpoint):\n",
    "        \n",
    "        overall_start = timer()\n",
    "        #SETTINGS: OPTIMIZER & SCHEDULER\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0, min_lr=1e-6, verbose=True)\n",
    "        #SETTINGS: LOSS\n",
    "        loss = torch.nn.BCEWithLogitsLoss()\n",
    "        #loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        #LOAD CHECKPOINT \n",
    "        if checkpoint != None and use_gpu:\n",
    "            modelCheckpoint = torch.load(checkpoint)\n",
    "            model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "            optimizer.load_state_dict(modelCheckpoint['optimizer'])\n",
    "\n",
    "        \n",
    "        #TRAIN THE NETWORK\n",
    "        lossMIN = 100000\n",
    "        start = timer()\n",
    "        dir_path = dir_str+launchTimestamp\n",
    "        #os.mkdir(dir_path)\n",
    "        for epochID in range(0, trMaxEpoch):\n",
    "            \n",
    "            timestampTime = time.strftime(\"%H%M%S\")\n",
    "            timestampDate = time.strftime(\"%d%m%Y\")\n",
    "            timestampSTART = timestampDate + '-' + timestampTime\n",
    "            \n",
    "            batchs, losst = Model_Trainer.epochTrain(model, dataLoaderTrain, optimizer, trMaxEpoch, epochID, nnClassCount, loss, start, lossMIN, launchTimestamp, dir_path)\n",
    "            outLoss, ground_truth, prediction, aurocMean, aurocIndividual = Model_Trainer.test(model, dataLoaderVal, nnClassCount, class_names, loss)\n",
    "   \n",
    "            outLoss = outLoss.cpu().detach().numpy()\n",
    "            print(\"\\nval loss: \" + str(outLoss))\n",
    "\n",
    "            timestampTime = time.strftime(\"%H%M%S\")\n",
    "            timestampDate = time.strftime(\"%d%m%Y\")\n",
    "            timestampEND = timestampDate + '-' + timestampTime\n",
    "            scheduler.step(outLoss)\n",
    "            \n",
    "            torch.save({'epoch': epochID + 1, 'state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict()}, 'save_bad_exam_models/bad_exam_detection_' + timestampEND + '_val_loss-' + str(outLoss) + '.pth.tar')   \n",
    "   \n",
    "        return batchs, losst        \n",
    "    #-------------------------------------------------------------------------------- \n",
    "       \n",
    "    def epochTrain(model, dataLoader, optimizer, epochMax, epochID, classCount, loss, start, lossMIN, launchTimestamp, dir_path):\n",
    "        \n",
    "        batch = []\n",
    "        losstrain = []\n",
    "        losseval = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for batchID, (varInput, target) in enumerate(dataLoaderTrain):\n",
    "            \n",
    "            target = torch.squeeze(target)\n",
    "            target = torch.ravel(target)\n",
    "            varInput = varInput.cuda()\n",
    "            varTarget = target.cuda(non_blocking = True)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                varOutput = model(varInput)\n",
    "                varOutput = np.squeeze(varOutput)\n",
    "                lossvalue = loss(varOutput, varTarget)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(lossvalue).backward()#lossvalue.backward()\n",
    "            scaler.step(optimizer)#optimizer.step()\n",
    "            scaler.update()\n",
    "          \n",
    "            l = lossvalue.item()\n",
    "            losstrain.append(l)\n",
    "            print(\n",
    "                f'Epoch: {epochID}\\t{100 * (batchID / (len(datasetTrain)//training_BatchSize)):.1f}% complete. {timer() - start:.1f} seconds elapsed in epoch. Training loss: ' + str(round(np.mean(losstrain), 4)),\n",
    "                end='\\r')\n",
    "           \n",
    "        return batch, losstrain\n",
    "    \n",
    "    \n",
    "    def epochVal(model, dataLoader, optimizer, loss):\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        lossVal = 0 \n",
    "        lossValNorm = 0\n",
    "        \n",
    "        outGT = torch.FloatTensor().cuda()\n",
    "        outPRED = torch.FloatTensor().cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (varInput, target) in enumerate(dataLoader):\n",
    "                \n",
    "                \n",
    "                target = torch.FloatTensor(target)\n",
    "                target = target.cuda(non_blocking = True)\n",
    "                \n",
    "                varOutput = model(varInput)\n",
    "                \n",
    "                losstensor = loss(varOutput, target)\n",
    "                \n",
    "                lossVal += losstensor\n",
    "                lossValNorm += 1\n",
    "                                \n",
    "                outGT = torch.cat((outGT, target), 0).cuda()\n",
    "                outPRED = torch.cat((outPRED, varOutput), 0)\n",
    "                \n",
    "                print(\n",
    "                    f'Epoch: {epochID}\\t{100 * (batchID / (len(dataLoader)//training_BatchSize)):.1f}% complete. {timer() - start:.1f} seconds elapsed in epoch. Training loss: ' + str(round(np.mean(losstrain), 4)),\n",
    "                    end='\\r')\n",
    "           \n",
    "        \n",
    "                \n",
    "        \n",
    "        outLoss = lossVal / lossValNorm\n",
    "        \n",
    "        print(\"val_loss: \" + str(outLoss))\n",
    "           \n",
    "        return outLoss, outGT, outPRED\n",
    "    \n",
    "\n",
    "    \n",
    "    def computeAUROC (dataGT, dataPRED, classCount):\n",
    "        \n",
    "        outAUROC = []\n",
    "        \n",
    "        datanpGT = dataGT.cpu().numpy()\n",
    "        datanpPRED = dataPRED.cpu().numpy()\n",
    "\n",
    "        outAUROC.append(roc_auc_score(datanpGT, datanpPRED))\n",
    "\n",
    "        return outAUROC\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def test(model, dataLoaderTest, nnClassCount, class_names, loss):   \n",
    "        \n",
    "        outGT = torch.FloatTensor().cuda()\n",
    "        outPRED = torch.FloatTensor().cuda()\n",
    "\n",
    "       \n",
    "        model.eval()\n",
    "        \n",
    "        lossVal = 0\n",
    "        lossValNorm = 0\n",
    "        \n",
    "        batch_num = 0\n",
    "        div = len(dataLoaderTest)\n",
    "        with torch.no_grad():\n",
    "            for i, (input, target) in enumerate(dataLoaderTest):\n",
    "                \n",
    "                target = torch.squeeze(target)\n",
    "                target = torch.ravel(target)\n",
    "                target = target.cuda()\n",
    "                outGT = torch.cat((outGT, target), 0).cuda()\n",
    "\n",
    "                bs, c, h, w = input.size()\n",
    "                varInput = input.view(-1, c, h, w)\n",
    "            \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    varOutput = model(varInput)\n",
    "                    varOutput = np.squeeze(varOutput)\n",
    "                    losstensor = loss(varOutput, target)   \n",
    "                                \n",
    "                lossVal += losstensor\n",
    "                lossValNorm += 1\n",
    "                \n",
    "                outPRED = torch.cat((outPRED, varOutput), 0)\n",
    "                \n",
    "                print(\n",
    "                    str(round(batch_num/div,3)),\n",
    "                    end='\\r')\n",
    "               \n",
    "                batch_num = batch_num + 1\n",
    "                \n",
    "                \n",
    "        outLoss = lossVal / lossValNorm \n",
    "        \n",
    "        aurocIndividual = Model_Trainer.computeAUROC(outGT, outPRED, nnClassCount)\n",
    "        aurocMean = np.array(aurocIndividual).mean()\n",
    "        \n",
    "        print ('\\nAUROC mean ', round(aurocMean, 5))\n",
    "        \n",
    "        for i in range (0, len(aurocIndividual)):\n",
    "            print (class_names[i], ' ', round(aurocIndividual[i],5))\n",
    "            \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        return outLoss, outGT, outPRED, aurocMean, aurocIndividual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08835a5-eea1-4755-998d-5ac6624a3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320', pretrained=True, in_chans=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b1c51-41f0-4021-846e-64e9413d5bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = model.head.fc.in_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2d836-6e1a-4ae8-92b0-56399d403ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, len(y_list), bias=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534f9df-ff48-4730-b58d-2ddbfcecf450",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80754bb-6291-4de4-bd13-8fbcb2ce82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnClassCount = len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4899c3-1d88-4e3a-8d86-7edaa61e6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "timestampTime = time.strftime(\"%H%M%S\")\n",
    "timestampDate = time.strftime(\"%d%m%Y\")\n",
    "timestampLaunch = timestampDate + '-' + timestampTime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e69d5e-d5d5-4b17-acb6-f2d7b90f0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_str = 'bilateral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de5336-5e5f-410c-8536-0c38742fa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, losst = Model_Trainer.train(model, dataLoaderTrain, dataLoaderVal, nnClassCount, trMaxEpoch, timestampLaunch, checkpoint = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e903eb-71c7-497a-abf9-ebbfe31b7bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187d3b9-410d-45cf-8ffc-7b39c7f42fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
